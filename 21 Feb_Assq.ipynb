{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d35d0ad-fba5-40c2-a48d-998f43086f95",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7293dcc5-6031-4827-bf75-ab1c1819488e",
   "metadata": {},
   "source": [
    "Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. \n",
    "\n",
    "Data scraping has numerous applications across many industries—including insurance, banking, finance, trading, eCommerce, sports, and digital marketing. Data is also used to inform decision-making, generate leads and sales, manage risks, guide strategies, and create new products and services.\n",
    "\n",
    "three areas where Web Scraping is used to get data:\n",
    "1. Email Marketing\n",
    "2. News Monitoring\n",
    "3. Market Research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b93dc-84e9-4fb0-aa63-533e052500f3",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f26bcf-f2b6-4c9b-94b5-58553c3a76be",
   "metadata": {},
   "source": [
    "1. Human Copy-and-Paste: Manually copying and pasting data from a web page into a text file or spreadsheet is the most basic form of web scraping. Even the best web-scraping technology cannot always replace a human’s manual examination and copy-and-paste, and this may be the only viable option when the websites for scraping explicitly prohibit machine automation.\n",
    "\n",
    "2. Text Pattern Matching: The UNIX grep command or regular expression-matching facilities of programming languages can be used to extract information from web pages in a simple yet powerful way (for instance Perl or Python).\n",
    "\n",
    "3. HTML Parsing: Many websites contain large collections of pages that are dynamically generated from an underlying structured source, such as a database. A common script or template is typically used to encode data from the same category into similar pages. A wrapper is a program in data mining that detects such templates in a specific information source, extracts its content, and converts it to a relational form.\n",
    "\n",
    "4. DOM Parsing: More information: Object Model for Documents, Programs can retrieve dynamic content generated by client-side scripts by embedding a full-fledged web browser, such as Internet Explorer or the Mozilla browser control. These browser controls also parse web pages into a DOM tree, which programs can use to retrieve portions of the pages. The resulting DOM tree can be parsed using languages such as Xpath.\n",
    "\n",
    "5. Vertical Aggregation: Several companies have created vertically specific harvesting platforms. These platforms generate and monitor a plethora of “bots” for specific verticals with no “man in the loop” (direct human involvement) and no work related to a specific target site. The preparation entails creating a knowledge base for the entire vertical, after which the platform will create the bots automatically.\n",
    "\n",
    "6. Semantic Annotation Recognizing: The scraped pages may include metadata, semantic markups, and annotations that can be used to locate specific data snippets. This technique can be viewed as a subset of DOM parsing if the annotations are embedded in the pages, as Microformat does. In another case, the annotations are stored and managed separately from the web pages, so scrapers can retrieve data schema and instructions from this layer before scraping the pages.\n",
    "\n",
    "7. Computer Vision Web-Page Analysis: There are efforts using machine learning and computer vision to identify and extract information from web pages by visually interpreting pages as a human would."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad38f9-1154-45c7-9887-97179beae147",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b3e2b9-bd5a-4556-b423-80e9123120b2",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). \n",
    "\n",
    "It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e9e1e0-57b8-4cc7-ac54-a0969b7b8d1a",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd61c6f-df9d-4077-b5cb-1f6f62c31fe9",
   "metadata": {},
   "source": [
    "Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape. The first line imports the Flask class and the render_template method from the flask library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56831e8f-4045-4153-8447-97c0e839fb17",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9579d3e6-72b7-4f03-afbe-c820c01da53e",
   "metadata": {},
   "source": [
    "AWS Elastic Beanstalk\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
